{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data= np.array([[0,0],[0,1],[1,0],[1,1]], dtype=np.float32)\n",
    "y_data= np.array([[0],[1],[1],[0]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "W = tf.Variable(tf.random_normal([2,1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "#Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X,W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X,W)+b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost/Loss Function\n",
    "cost = -tf.reduce_mean(Y* tf.log(hypothesis) + (1-Y)* tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7159825 [[-0.24862091]\n",
      " [-0.57306033]]\n",
      "100 0.69603693 [[-0.10563701]\n",
      " [-0.27934745]]\n",
      "200 0.69438434 [[-0.08210842]\n",
      " [-0.17493911]]\n",
      "300 0.6936886 [[-0.06196496]\n",
      " [-0.11156259]]\n",
      "400 0.6933874 [[-0.04532611]\n",
      " [-0.07182305]]\n",
      "500 0.69325477 [[-0.03246192]\n",
      " [-0.04661719]]\n",
      "600 0.6931957 [[-0.02290796]\n",
      " [-0.03046995]]\n",
      "700 0.6931691 [[-0.01599457]\n",
      " [-0.02003431]]\n",
      "800 0.69315714 [[-0.01108017]\n",
      " [-0.01323825]]\n",
      "900 0.6931517 [[-0.00763063]\n",
      " [-0.0087835 ]]\n",
      "1000 0.6931492 [[-0.00523154]\n",
      " [-0.00584742]]\n",
      "1100 0.69314814 [[-0.00357445]\n",
      " [-0.00390347]]\n",
      "1200 0.6931476 [[-0.00243578]\n",
      " [-0.00261153]]\n",
      "1300 0.6931474 [[-0.00165643]\n",
      " [-0.00175032]]\n",
      "1400 0.6931473 [[-0.00112464]\n",
      " [-0.00117481]]\n",
      "1500 0.69314724 [[-0.00076262]\n",
      " [-0.00078943]]\n",
      "1600 0.6931472 [[-0.00051663]\n",
      " [-0.00053095]]\n",
      "1700 0.69314724 [[-0.00034969]\n",
      " [-0.00035733]]\n",
      "1800 0.6931472 [[-0.00023659]\n",
      " [-0.00024067]]\n",
      "1900 0.6931472 [[-0.00015998]\n",
      " [-0.00016216]]\n",
      "2000 0.6931472 [[-0.00010814]\n",
      " [-0.0001093 ]]\n",
      "2100 0.6931472 [[-7.306978e-05]\n",
      " [-7.368802e-05]]\n",
      "2200 0.6931472 [[-4.9360540e-05]\n",
      " [-4.9691174e-05]]\n",
      "2300 0.6931471 [[-3.3335829e-05]\n",
      " [-3.3514476e-05]]\n",
      "2400 0.6931472 [[-2.2507147e-05]\n",
      " [-2.2605338e-05]]\n",
      "2500 0.6931472 [[-1.5190677e-05]\n",
      " [-1.5244164e-05]]\n",
      "2600 0.6931472 [[-1.0250938e-05]\n",
      " [-1.0276113e-05]]\n",
      "2700 0.6931472 [[-6.922026e-06]\n",
      " [-6.933789e-06]]\n",
      "2800 0.6931472 [[-4.6734390e-06]\n",
      " [-4.6822215e-06]]\n",
      "2900 0.6931472 [[-3.1505388e-06]\n",
      " [-3.1503823e-06]]\n",
      "3000 0.6931472 [[-2.1283138e-06]\n",
      " [-2.1281573e-06]]\n",
      "3100 0.6931472 [[-1.4175267e-06]\n",
      " [-1.4173703e-06]]\n",
      "3200 0.6931472 [[-9.704873e-07]\n",
      " [-9.703309e-07]]\n",
      "3300 0.69314724 [[-6.5905249e-07]\n",
      " [-6.5889606e-07]]\n",
      "3400 0.6931472 [[-4.2361393e-07]\n",
      " [-4.2345749e-07]]\n",
      "3500 0.69314724 [[-2.6566101e-07]\n",
      " [-2.6550458e-07]]\n",
      "3600 0.6931472 [[-2.0903639e-07]\n",
      " [-2.0887995e-07]]\n",
      "3700 0.6931472 [[-1.5986237e-07]\n",
      " [-1.5970593e-07]]\n",
      "3800 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "3900 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "4000 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "4100 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "4200 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "4300 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "4400 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "4500 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "4600 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "4700 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "4800 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "4900 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "5000 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "5100 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "5200 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "5300 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "5400 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "5500 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "5600 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "5700 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "5800 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "5900 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "6000 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "6100 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "6200 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "6300 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "6400 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "6500 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "6600 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "6700 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "6800 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "6900 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "7000 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "7100 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "7200 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "7300 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "7400 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "7500 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "7600 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "7700 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "7800 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "7900 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "8000 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "8100 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "8200 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "8300 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "8400 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "8500 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "8600 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "8700 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "8800 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "8900 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "9000 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "9100 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "9200 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "9300 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "9400 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "9500 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "9600 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "9700 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "9800 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "9900 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "10000 0.6931472 [[-1.3304017e-07]\n",
      " [-1.3288374e-07]]\n",
      "\n",
      "Hypothesis:  [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "Accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "#Accuracy Computation\n",
    "#True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "#launch graph\n",
    "with tf.Session() as sess:\n",
    "    #Initialize Tensorflow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n",
    "            \n",
    "    #Accuracy Report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Net\n",
    "W = tf.Variable(tf.random_normal([2,1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "#Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X,W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X,W)+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random_normal([2,2]), name='weight')\n",
    "b1 = tf.Variable(tf.random_normal([2]), name='bias')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1)+ b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([2,1]), name='weight')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2)+ b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost/Loss Function\n",
    "cost = -tf.reduce_mean(Y* tf.log(hypothesis) + (1-Y)* tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7624923 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "100 0.6996664 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "200 0.6978523 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "300 0.6966155 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "400 0.6957422 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "500 0.69510096 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "600 0.6946093 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "700 0.6942137 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "800 0.69387937 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "900 0.6935817 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "1000 0.6933029 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "1100 0.69302887 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "1200 0.69274694 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "1300 0.69244504 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "1400 0.69211006 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "1500 0.69172716 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "1600 0.69127846 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "1700 0.69074166 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "1800 0.6900887 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "1900 0.68928266 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "2000 0.68827593 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "2100 0.6870048 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "2200 0.68538517 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "2300 0.683306 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "2400 0.6806223 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "2500 0.67715025 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "2600 0.6726657 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "2700 0.66691196 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "2800 0.65962136 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "2900 0.65055096 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "3000 0.63952625 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "3100 0.6264654 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "3200 0.61135906 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "3300 0.59418863 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "3400 0.5747982 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "3500 0.55277455 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "3600 0.5274134 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "3700 0.49787462 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "3800 0.46359468 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "3900 0.42484814 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "4000 0.38308993 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "4100 0.3406883 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "4200 0.30013013 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "4300 0.2632499 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "4400 0.23093727 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "4500 0.20329002 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "4600 0.17993234 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "4700 0.16029163 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "4800 0.14376688 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "4900 0.1298109 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "5000 0.117957674 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "5100 0.10782411 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "5200 0.099100694 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "5300 0.0915395 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "5400 0.0849414 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "5500 0.07914687 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "5600 0.07402729 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "5700 0.06947831 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "5800 0.06541494 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "5900 0.061767373 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "6000 0.058478057 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "6100 0.05549906 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "6200 0.05279041 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "6300 0.050318427 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "6400 0.048054557 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "6500 0.04597453 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "6600 0.044057705 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "6700 0.042286225 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "6800 0.04064472 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "6900 0.039119802 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "7000 0.037699938 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "7100 0.036374852 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "7200 0.03513569 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "7300 0.033974577 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "7400 0.03288453 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "7500 0.031859487 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "7600 0.030893803 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "7700 0.029982759 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "7800 0.029121831 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "7900 0.028307166 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "8000 0.027535195 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "8100 0.026802678 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "8200 0.026106805 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "8300 0.025444899 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "8400 0.024814617 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "8500 0.024213763 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "8600 0.023640394 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "8700 0.023092685 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "8800 0.02256899 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "8900 0.022067819 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "9000 0.021587687 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "9100 0.021127438 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "9200 0.020685792 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "9300 0.020261705 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "9400 0.019854255 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "9500 0.019462306 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "9600 0.01908513 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "9700 0.018721886 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "9800 0.018371891 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "9900 0.01803437 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "10000 0.017708704 [[-1.1502247 ]\n",
      " [ 0.17129683]]\n",
      "\n",
      "Hypothesis:  [[0.01713874]\n",
      " [0.98483354]\n",
      " [0.9765955 ]\n",
      " [0.01447636]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "#Accuracy Computation\n",
    "#True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "#launch graph\n",
    "with tf.Session() as sess:\n",
    "    #Initialize Tensorflow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n",
    "            \n",
    "    #Accuracy Report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wide NN for XOR\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2,10]), name='weight')\n",
    "b1 = tf.Variable(tf.random_normal([10]), name='bias')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1)+ b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10,1]), name='weight')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2)+ b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost/Loss Function\n",
    "cost = -tf.reduce_mean(Y* tf.log(hypothesis) + (1-Y)* tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.73202455 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "100 0.68746394 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "200 0.6718108 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "300 0.6579114 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "400 0.6438293 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "500 0.6286537 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "600 0.6118829 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "700 0.59317505 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "800 0.57227266 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "900 0.54900336 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "1000 0.5233108 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "1100 0.4952808 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "1200 0.465136 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "1300 0.43322223 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "1400 0.40002716 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "1500 0.36620373 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "1600 0.33253253 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "1700 0.29982394 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "1800 0.26880044 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "1900 0.24000779 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "2000 0.21377903 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "2100 0.19024694 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "2200 0.16938403 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "2300 0.15104932 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "2400 0.13503373 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "2500 0.12109536 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "2600 0.10898548 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "2700 0.09846561 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "2800 0.08931655 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "2900 0.08134343 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "3000 0.074375875 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "3100 0.06826746 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "3200 0.06289325 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "3300 0.05814752 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "3400 0.053940743 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "3500 0.05019758 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "3600 0.0468544 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "3700 0.04385741 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "3800 0.041161157 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "3900 0.03872714 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "4000 0.036522467 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "4100 0.034519248 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "4200 0.032693405 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "4300 0.031024477 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "4400 0.02949477 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "4500 0.028088886 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "4600 0.02679356 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "4700 0.025597274 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "4800 0.024489846 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "4900 0.023462508 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "5000 0.022507409 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "5100 0.02161781 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "5200 0.020787546 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "5300 0.020011293 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "5400 0.01928433 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "5500 0.018602302 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "5600 0.017961564 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "5700 0.01735859 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "5800 0.016790368 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "5900 0.016254181 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "6000 0.01574754 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "6100 0.015268158 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "6200 0.014814071 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "6300 0.014383445 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "6400 0.01397457 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "6500 0.01358592 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "6600 0.01321616 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "6700 0.012864028 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "6800 0.012528222 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "6900 0.012207864 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "7000 0.011901883 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "7100 0.011609377 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "7200 0.011329518 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "7300 0.011061586 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "7400 0.010804804 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "7500 0.010558622 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "7600 0.010322306 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "7700 0.010095383 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "7800 0.00987732 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "7900 0.009667629 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "8000 0.009465855 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "8100 0.009271553 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "8200 0.009084346 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "8300 0.008903941 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "8400 0.008729886 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "8500 0.00856192 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "8600 0.008399741 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "8700 0.008243074 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "8800 0.0080916155 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "8900 0.007945155 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "9000 0.0078034764 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "9100 0.0076662963 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "9200 0.007533475 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "9300 0.0074047865 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "9400 0.007280078 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "9500 0.0071591237 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "9600 0.0070418166 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "9700 0.006927946 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "9800 0.00681739 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "9900 0.006710072 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "10000 0.006605752 [[ 0.80078834]\n",
      " [-1.5733615 ]]\n",
      "\n",
      "Hypothesis:  [[0.00490994]\n",
      " [0.9934082 ]\n",
      " [0.9930213 ]\n",
      " [0.00785321]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "#Accuracy Computation\n",
    "#True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "#launch graph\n",
    "with tf.Session() as sess:\n",
    "    #Initialize Tensorflow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n",
    "            \n",
    "    #Accuracy Report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep NN for XOR\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2,10]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([10]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1)+ b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10,10]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([10]), name='bias2')\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, W2)+ b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([10,10]), name='weight3')\n",
    "b3 = tf.Variable(tf.random_normal([10]), name='bias3')\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, W3)+ b3)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([10,1]), name='weight4')\n",
    "b4 = tf.Variable(tf.random_normal([1]), name='bias4')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer3, W4)+ b4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost/Loss Function\n",
    "cost = -tf.reduce_mean(Y* tf.log(hypothesis) + (1-Y)* tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7007752 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "100 0.691582 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "200 0.68848145 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "300 0.6849528 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "400 0.6806163 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "500 0.675016 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "600 0.66762775 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "700 0.65803343 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "800 0.6461971 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "900 0.63239473 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "1000 0.6169862 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "1100 0.60052705 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "1200 0.58379984 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "1300 0.5675273 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "1400 0.5520241 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "1500 0.53695786 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "1600 0.52113485 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "1700 0.50222456 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "1800 0.47670197 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "1900 0.43917492 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "2000 0.38135862 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "2100 0.30336025 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "2200 0.22219017 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "2300 0.1569953 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "2400 0.11263294 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "2500 0.083803214 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "2600 0.064749874 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "2700 0.051701486 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "2800 0.042426627 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "2900 0.035606947 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "3000 0.030442577 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "3100 0.026431434 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "3200 0.023247529 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "3300 0.020672485 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "3400 0.018555872 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "3500 0.016791284 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "3600 0.015301874 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "3700 0.014031002 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "3800 0.012935966 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "3900 0.0119842915 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "4000 0.011150772 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "4100 0.010415642 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "4200 0.009763193 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "4300 0.009180708 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "4400 0.008658085 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "4500 0.008186809 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "4600 0.0077599953 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "4700 0.007371909 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "4800 0.007017698 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "4900 0.0066932277 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "5000 0.0063951365 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "5100 0.006120385 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "5200 0.0058663897 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "5300 0.00563108 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "5400 0.005412478 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "5500 0.0052089095 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "5600 0.005018957 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "5700 0.0048413984 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "5800 0.004675 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "5900 0.0045188414 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "6000 0.0043719895 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "6100 0.0042337375 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "6200 0.0041033025 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "6300 0.0039800834 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "6400 0.0038635675 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "6500 0.0037531853 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "6600 0.00364847 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "6700 0.003549076 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "6800 0.0034545078 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "6900 0.003364585 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "7000 0.0032788427 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "7100 0.003197115 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "7200 0.003119057 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "7300 0.0030444588 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "7400 0.00297311 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "7500 0.0029048459 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "7600 0.0028393813 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "7700 0.0027766412 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "7800 0.0027164307 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "7900 0.002658585 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "8000 0.0026029993 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "8100 0.002549583 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "8200 0.0024980973 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "8300 0.0024485714 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "8400 0.0024007957 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "8500 0.0023548002 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "8600 0.0023103605 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "8700 0.0022675062 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "8800 0.0022260577 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "8900 0.0021860597 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "9000 0.0021473628 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "9100 0.002109922 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "9200 0.0020736922 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "9300 0.0020386137 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "9400 0.0020046264 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "9500 0.0019716711 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "9600 0.0019397468 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "9700 0.0019087195 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "9800 0.0018786488 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "9900 0.0018494749 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "10000 0.0018211231 [[-0.09110948]\n",
      " [-1.0249412 ]]\n",
      "\n",
      "Hypothesis:  [[0.00143937]\n",
      " [0.9980697 ]\n",
      " [0.9985128 ]\n",
      " [0.00242065]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "#Accuracy Computation\n",
    "#True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "#launch graph\n",
    "with tf.Session() as sess:\n",
    "    #Initialize Tensorflow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n",
    "            \n",
    "    #Accuracy Report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
